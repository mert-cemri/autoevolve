# Configuration for Real-Time Adaptive Signal Processing Example
max_iterations: 100
checkpoint_interval: 1
log_level: "INFO"

# LLM configuration
llm:
  primary_model: "gpt-5"
  api_base: "https://api.openai.com/v1"
  primary_model_weight: 1.0
  temperature: 0.7
  top_p: 0.95
  max_tokens: 32000
  timeout: 900

# Prompt configuration
prompt:
  system_message: "You are an expert for model placement on GPUs. Your task is to improve a model placement algorithm by improve the function named compute_model_placement in the intial program that places models to available GPUs. 
  The algorithm must MINIMIZE the maximum KVPR across all GPUs while ensuring models can fit into the GPUs' memory. Note that KVPR is KV cache pressure for a GPU. It indicates how crowded a GPU is. For a specific GPU, its KVPR is computed as sum(model.req_rate/model.slo for model in models) / (GPU_MEM_SIZE - sum(model.model_size for model in models)), where models are the models on this GPU. The generated program should be as simple as possible and the code should be executed correctly without errors."
  num_top_programs: 3
  use_template_stochasticity: true

# Database configuration
database:
  population_size: 80
  archive_size: 30
  num_islands: 4
  elite_selection_ratio: 0.15
  exploitation_ratio: 0.65

  # Adaptive exploration settings
  use_adaptive_search: true
  adaptive_window_size: 20
  adaptive_min_exploration: 0.1
  adaptive_max_exploration: 0.7

  # Softmax sampling for exploitation
  exploitation_temperature: 1.0  # Controls sharpness of softmax (lower = more greedy)

  # Stagnation detection and multi-child generation
  stagnation_threshold: 10  # Iterations without improvement to trigger stagnation
  stagnation_multi_child_count: 3  # Number of diverse children to generate on stagnation

  # Sibling context for prompts
  sibling_context_limit: 5  # Number of previous children to show in prompt
  
# Evaluator configuration
evaluator:
  timeout: 90
  cascade_evaluation: false
  cascade_thresholds: [0.3, 0.6]
  parallel_evaluations: 4
  use_llm_feedback: false

# Evolution settings
diff_based_evolution: true
max_code_length: 60000
